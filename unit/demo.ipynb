{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "import numpy as np\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import *\n",
    "from datasets import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting hyperparameters (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 64\n",
    "n_downsample = 2\n",
    "channels = 3\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "shared_dim = dim * 2 ** n_downsample\n",
    "input_shape = (channels, img_height, img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_E = ResidualBlock(features=shared_dim).cuda()\n",
    "E1 = Encoder(dim=dim, n_downsample=n_downsample, shared_block=shared_E).cuda()\n",
    "E2 = Encoder(dim=dim, n_downsample=n_downsample, shared_block=shared_E).cuda()\n",
    "shared_G = ResidualBlock(features=shared_dim).cuda()\n",
    "G1 = Generator(dim=dim, n_upsample=n_downsample, shared_block=shared_G).cuda()\n",
    "G2 = Generator(dim=dim, n_upsample=n_downsample, shared_block=shared_G).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 180\n",
    "\n",
    "E1.load_state_dict(torch.load(\"./checkpoint/E1_%d.pth\" % (epoch), map_location='cuda:0')) # \n",
    "E2.load_state_dict(torch.load(\"./checkpoint/E2_%d.pth\" % (epoch), map_location='cuda:0'))\n",
    "G1.load_state_dict(torch.load(\"./checkpoint/G1_%d.pth\" % (epoch), map_location='cuda:0'))\n",
    "G2.load_state_dict(torch.load(\"./checkpoint/G2_%d.pth\" % (epoch), map_location='cuda:0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/FLIR_ADAS_v2/video_rgb_test/data/video-BzZspxAweF8AnKhWK-frame-000745-SSCRtAHcFjphNPczJ.jpg\n",
      "/data/FLIR_ADAS_v2/video_thermal_test/data/video-4FRnNpmSmwktFJKjg-frame-000745-L6K5SC6fYjHNC8uff.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xieshaoyuan/anaconda3/envs/pointnet/lib/python3.6/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "root = '/data/FLIR_ADAS_v2'\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(img_height * 1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "dataset = ImageDataset(root=root, transforms_=transforms_, unaligned=False, mode='test')\n",
    "dataloader = DataLoader(dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "print(dataset.files_A[0])\n",
    "print(dataset.files_B[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 3. Got 304 and 306 (The offending index is 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-29a84bd17e98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mfake_X2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mimg_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_X2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/%s.png\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatches_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 3. Got 304 and 306 (The offending index is 0)"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "save_path = 'images/test/'\n",
    "if not os.path.isdir(save_path): os.makedirs(save_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batches_done, imgs in enumerate(dataloader):\n",
    "        X1 = imgs[\"A\"].cuda()\n",
    "        X2 = imgs[\"B\"].cuda()\n",
    "        _, Z2 = E2(X1)\n",
    "        fake_X2 = G1(Z2)\n",
    "        img_sample = torch.cat((X1.data, X2.data, fake_X2.data), 0)\n",
    "        save_image(img_sample, save_path+\"/%s.png\" % (batches_done), nrow=5, normalize=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7dc874a9331343f0864306bcd1650a7174b6bd5872c46903dd8126c4088a9e3"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pointnet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
